{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Eigenvalues, Eigenvectors, and the Rayleigh Quotient\n",
        "\n",
        "In lecture you have been discussing eigenvectors and Eigenvalues, diagonalizable matrices, spectral decomposition, power iterations (Lanczos), and generalizing the idea of functions to matrices.\n",
        "\n",
        "Essentially, the eigenvectors are an orthonormal basis for your matrix (remember section 3?) and the Eigenvalues describe the characteristic of the matrix.\n",
        "\n",
        "In this section, we will use the spectral decomposition and the singular value decomposition as a tool for dimensionality reduction and compression."
      ],
      "metadata": {
        "id": "RHh-ILUg49kP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principle Component Analysis (PCA)\n",
        "\n",
        "Here we will learn how to use `scipy` to decompose a matrix into its spectral components and use those components to describe the matrix.\n",
        "\n",
        "One common technique in data analysis is called Principle Component Analysis (PCA). This technique involves the spectral decomposition of the covariance matrix of the data. In the next section, we will see how this relates to the Singular Value Decomposition of the data matrix.\n",
        "\n",
        "Let's proceed by taking a subset of the MNIST observations. In the code below, I have filtered the train data to only have 'ones' and 'fives' in the new dataset. Then, I have calculated the covariance of this new data and decomposed it into eigenvectors and values using `scipy`.\n",
        "\n",
        "The eigenvectors of a covariance are referred to as 'principle axes' and the magnitude of the projections of observations onto these axes are referred to as 'principle components' in PCA. These principle axes for a basis for a low dimensional space which we project the data into."
      ],
      "metadata": {
        "id": "b7bnDspWB7t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from IPython.display import HTML\n",
        "from matplotlib import animation\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape((60000, 784))\n",
        "\n",
        "filter = np.logical_or((y_train == 1), (y_train == 5))\n",
        "data = x_train[filter]\n",
        "labels = y_train[filter]\n",
        "\n",
        "print(f'shapes: {data.shape} and {labels.shape}')\n",
        "\n",
        "cov = np.cov(data, rowvar=False)\n",
        "eigs, Q = scipy.linalg.eigh(cov)\n",
        "x = np.arange(len(eigs))\n",
        "\n",
        "plt.scatter(x, eigs)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, eigs)\n",
        "plt.yscale('symlog')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MH2Ux0aRkCw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I have plotted the eigenvalues with a linear and log y axis. For PCA to be effective, we would like to capture most of the 'information' with less eigenvectors than dimension of our observations.\n",
        "\n",
        "These plots show that the first few largest magnitude eigenvectors (last 3 columns of `Q`) can mostly describe the covariance between pixels in our dataset."
      ],
      "metadata": {
        "id": "SVlfM0gA5R4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "principle_components = data @ Q[:,-3:]\n",
        "\n",
        "# 3d projection\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(principle_components[:,0], principle_components[:,1], principle_components[:,2], c=labels, alpha=0.2)\n",
        "plt.show()\n",
        "\n",
        "# NOTE: producing the animated video takes a long time... (about 2 minutes)\n",
        "# set this to False for stationary projection\n",
        "animate_plot = True\n",
        "\n",
        "if animate_plot:\n",
        "  def animate(frame):\n",
        "    ax.view_init(30, frame)\n",
        "    plt.pause(.001)\n",
        "    return fig\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=360, interval=50)"
      ],
      "metadata": {
        "id": "LANCAuyA7TaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING: this is the block that actually takes a long time\n",
        "# Give it a couple minutes....\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "eBnTV8EbjQIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2d projection\n",
        "plt.scatter(principle_components[:,1], principle_components[:,2], c=labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2KcOlTQEeLkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I have plotted the data matrix projected into a 3 dimensional and 2 dimensional space using the principle axes as a basis. The color denotes if it is a 1 or a 5. We can see a clear decision boundary between the two classes we selected. Based on the distribution of the scatter, can you guess which color is which label?\n",
        "\n",
        "Often times in machine learning we would like to find a decision boundary to classify observations in a high dimensional space (like MNIST, dimension 784). It is much easier to model lower dimensional data. Instead of modeling the raw images we could learn to classify an observation based on its principle components. This is the power of dimensionality reduction techniques such as PCA."
      ],
      "metadata": {
        "id": "HjRi9XdsA8yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework 4.1\n",
        "\n",
        "*   Calculate and plot the eigenvalues when selecting 3 or more different classes of observations on top of the previous plot. Comment on how the distribution of the spectrum differs from the example with only 2 observations (1 and 5).\n",
        "*   Are there classes in this data set you think wouldn't demonstrate such a clean decision boundary? Plot the low dimensional embeddings for these classes and see if they have more overlap."
      ],
      "metadata": {
        "id": "B12Mq94ODXpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "jJID3oMLE8M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Truncated SVD\n",
        "\n",
        "Often times we have matrix data that is stored in a low rank format. We can compress the matrix in a lossy or lossless way using truncated SVD to require less memory.\n",
        "\n",
        "Let's look to our old friend, MNIST, for a practical example. MNIST naturally has a lot of row and column dependency. Remember that in MNIST, rows are observations and columns correspond to individual pixels.\n",
        "\n",
        "Some examples of column dependency in our data is the corners of the images are likely white and full of zeros. These corners could be removed from the dataset without much loss (hence the dependency).\n",
        "\n",
        "Row dependency is produced because there are only 10 classes within 60,000 images. Images within a class are quite similar and produce the row dependence.\n",
        "\n",
        "We could use truncated SVD to compress our dataset and eliminate much of the dependence."
      ],
      "metadata": {
        "id": "SUNOj1NZGDtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape((60000, 784))\n",
        "\n",
        "U, s, V_t = scipy.linalg.svd(x_train, full_matrices=False)"
      ],
      "metadata": {
        "id": "OAEvkyLMJfsk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(len(s))\n",
        "\n",
        "plt.scatter(x, s)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(x, s)\n",
        "plt.yscale('symlog')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gyIrdr7hJ6wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By plotting the singular values much like we did the eigenvalues in the previous section we see that there is only a small amount of *strictly* dependent data but a significant amount that is 'almost dependent'.\n",
        "\n",
        "I'll try taking the first 50 rows of the left vectors and the first 50 columns of the right vectors (only 12.7% of the size of the original data)."
      ],
      "metadata": {
        "id": "1naWPZspKMXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size = 50\n",
        "reconstruction = U[:,:size] @ np.diag(s[:size]) @ V_t[:size,:]\n",
        "\n",
        "# plot some random images to see how they look\n",
        "ids = np.random.randint(60000, size=10)\n",
        "\n",
        "fig, axs = plt.subplots(2, 10)\n",
        "\n",
        "\n",
        "for i, id in enumerate(ids):\n",
        "  original = x_train[id]# plot the sample\n",
        "  compressed = reconstruction[id]\n",
        "  axs[0, i].imshow(original.reshape(28, 28), cmap='gray')\n",
        "  axs[1, i].imshow(compressed.reshape(28, 28), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "error = x_train - reconstruction\n",
        "sum_squared_error = np.linalg.norm(error.flatten())\n",
        "print(f'error norm: {sum_squared_error ** 0.5}')"
      ],
      "metadata": {
        "id": "sHkie7nDMDLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework 4.2\n",
        "\n",
        "*   Apply SVD to the data matrix but filtered on a single label. Truncate the SVD using the same number of vectors as I did above and reconstruct some images.\n",
        "\n",
        "*   When isolating the dataset to a single observation class, does the reconstruction improve?\n",
        "\n",
        "*   Calculate the error norm between your reconstructed data and the original dataset. Also filter the truncated `U` from my example on the same label you selected for the first two steps, reconstruct the data using that filtered `U` and my truncated `s` and `V_t`. Calculate the error of this reconstruction. Which is worse?\n",
        "\n",
        "*   Do the previous step in a loop over each label class and calculate the sum of the sum squared errors. Compare that to my total sum squared error. How much improvement is there? How much more total memory did you use?"
      ],
      "metadata": {
        "id": "Ie3ptHZ6T0wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "gpdXnhhAWHSk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
